# -*- coding: utf-8 -*-
"""shadow fox internship basic 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOThDgT6uMPiHFlp2WRMWeg43aTI0s9E
"""

pip install pandas numpy scikit-learn seaborn matplotlib

# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Correct the dataset URL or use a local file if the URL is unavailable
# Update the URL or replace with a local path to the Boston housing dataset
url = 'https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv'

# Load the dataset
try:
    data = pd.read_csv(url)
    print("Dataset loaded successfully.")
except Exception as e:
    print("Error loading dataset:", e)
    # Provide a fallback option for local dataset
    print("Ensure you provide the correct URL or local dataset file path.")
    raise

# Inspect the dataset
print("Dataset Head:")
print(data.head())
print("\nDataset Info:")
print(data.info())
print("\nSummary Statistics:")
print(data.describe())

# Data Preprocessing
# Handle missing values (if any)
if data.isnull().sum().sum() > 0:
    print("\nHandling missing values...")
    data.fillna(data.mean(), inplace=True)

# Check correlation heatmap to understand feature relationships
plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation")
plt.show()

# Define features (X) and target (y)
target = 'medv'  # Replace 'medv' with the actual target column name if it's different
X = data.drop(columns=[target])
y = data[target]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Model Training: Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Model Evaluation: Linear Regression
y_pred_lr = lr_model.predict(X_test)
print("\nLinear Regression Performance:")
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lr)))
print("R2 Score:", r2_score(y_test, y_pred_lr))

# Model Training: Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Model Evaluation: Random Forest
y_pred_rf = rf_model.predict(X_test)
print("\nRandom Forest Performance:")
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))
print("R2 Score:", r2_score(y_test, y_pred_rf))

# Feature Importance for Random Forest
plt.figure(figsize=(10, 6))
sns.barplot(x=rf_model.feature_importances_, y=X.columns)
plt.title("Feature Importance from Random Forest")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# Predict on new data (if available)
# Example: new_data = pd.DataFrame({'feature1': [value1], 'feature2': [value2], ...})
# scaled_new_data = scaler.transform(new_data)
# predictions = rf_model.predict(scaled_new_data)